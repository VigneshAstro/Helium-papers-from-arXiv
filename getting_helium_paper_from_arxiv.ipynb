{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f23a04e",
   "metadata": {},
   "source": [
    "# Searching the arxiv for helium papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8519b4d5",
   "metadata": {},
   "source": [
    "## First installing the arxiv package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "810ed0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting arxiv==1.3.0\n",
      "  Downloading arxiv-1.3.0-py3-none-any.whl (11 kB)\n",
      "Collecting feedparser (from arxiv==1.3.0)\n",
      "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sgmllib3k (from feedparser->arxiv==1.3.0)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=dbbf19b7de3836d01908e01b2f36bcabf7f66295142c9cebf2840d4d5e298d2c\n",
      "  Stored in directory: /Users/vigneshwarankrishnamurthy/Library/Caches/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
      "Successfully installed arxiv-1.3.0 feedparser-6.0.10 sgmllib3k-1.0.0\n"
     ]
    }
   ],
   "source": [
    "#installing arxiv\n",
    "!pip install arxiv==1.3.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84426960",
   "metadata": {},
   "source": [
    "## Before running the code, search http://exoplanet.eu/catalog/all_fields/ and download the csv file and save it as exoplanets.csv. This gives the updated parameters for all exoplanets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e198c2",
   "metadata": {},
   "source": [
    "## This code below searches for arxiv and finds the papers that are related to helium. It also removes papers that are not related to helium (example brown dwarf or white dwarf planets)Run this code for finding the paper and store it in a file called step_1_helium_5.csv.\n",
    "## Do not forget to change the starting date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96e7e26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code reads the arxiv files from the start date and updates the list and stores it in helium_4.csv\n",
    "\n",
    "import arxiv\n",
    "import datetime\n",
    "import re\n",
    "import csv\n",
    "\n",
    "# Read exoplanet names from the CSV file\n",
    "exoplanet_names = set()\n",
    "with open('exoplanets.csv', 'r') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        exoplanet_names.add(row['name'].strip())\n",
    "        alternate_names = row['alternate_names'].split(',')\n",
    "        for name in alternate_names:\n",
    "            exoplanet_names.add(name.strip())\n",
    "\n",
    "# Define the search query\n",
    "query = 'cat:astro-ph.EP AND (ti:\"helium\" OR ti:\"He\" OR abs:\"helium\" OR abs:\"He\" OR ti:\"1083\" OR ti:\"10833\" OR abs:\"1083\" OR abs:\"10833\")'\n",
    "start_date = datetime.date(2023, 7, 25) # searched until 2023 Nov 24\n",
    "\n",
    "# Search for papers\n",
    "search = arxiv.Search(\n",
    "    query=query,\n",
    "    sort_by=arxiv.SortCriterion.SubmittedDate\n",
    ")\n",
    "search_results = search.results()\n",
    "\n",
    "# Filter search results by publication date and keywords\n",
    "filtered_results = []\n",
    "for paper in search_results:\n",
    "    if paper.published.date() < start_date:\n",
    "        continue\n",
    "    if any(keyword in paper.title.lower() or keyword in paper.summary.lower() for keyword in [\"brown dwarf\", \"white dwarf\"]):\n",
    "        continue\n",
    "    filtered_results.append(paper)\n",
    "\n",
    "# Write the results to a CSV file\n",
    "with open('step_1_helium_5.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['arXiv ID', 'URL', 'Exoplanet Names Found'])\n",
    "\n",
    "    for paper in filtered_results:\n",
    "        arxiv_id = paper.entry_id.split('/')[-1]\n",
    "        url = f'https://arxiv.org/abs/{arxiv_id}'\n",
    "\n",
    "        # Find exoplanet names in the title and the abstract\n",
    "        exoplanet_names_title = {name for name in exoplanet_names if name in paper.title or name.replace(\" \", \"\") in paper.title}\n",
    "        exoplanet_names_abstract = {name for name in exoplanet_names if name in paper.summary or name.replace(\" \", \"\") in paper.summary}\n",
    "        exoplanet_names_found = exoplanet_names_title | exoplanet_names_abstract\n",
    "\n",
    "        # Write the result to the CSV file\n",
    "        writer.writerow([arxiv_id, url, ', '.join(exoplanet_names_found)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7045c7fd",
   "metadata": {},
   "source": [
    "## Instead of the running the above code, we can run the code below. It not just finds the papers with helium and also stores the type of the study (either simulated or observational- very preliminary results). \n",
    "## Do not forget to change the starting date and the saving file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4f90164",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code reads the arxiv files from the start date and updates the list and stores it in helium_4.csv\n",
    "# also it removes the papers with brown dwarfs and white dwarfs and categorises them in three types.\n",
    "\n",
    "import arxiv\n",
    "import datetime\n",
    "import re\n",
    "import csv\n",
    "\n",
    "# Read exoplanet names from the CSV file\n",
    "exoplanet_names = set()\n",
    "with open('exoplanets.csv', 'r') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        exoplanet_names.add(row['# name'].strip())\n",
    "        alternate_names = row['alternate_names'].split(',')\n",
    "        for name in alternate_names:\n",
    "            exoplanet_names.add(name.strip())\n",
    "\n",
    "# Read URLs from the helium_planets_april2023.csv file\n",
    "helium_planets_urls = set()\n",
    "with open('helium_planets_april2023.csv', 'r') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        urls = row['URL'].split(', ')\n",
    "        for url in urls:\n",
    "            helium_planets_urls.add(url.strip())\n",
    "\n",
    "# Define the search query\n",
    "query = 'cat:astro-ph.EP AND (ti:\"helium\" OR ti:\"He\" OR abs:\"helium\" OR abs:\"He\" OR ti:\"1083\" OR ti:\"10833\" OR abs:\"1083\" OR abs:\"10833\")'\n",
    "start_date = datetime.date(2023, 1, 1)\n",
    "\n",
    "# Define the observation and simulation keywords\n",
    "observation_keywords = ['observation', 'observations', 'observed', 'observing', 'observe']\n",
    "simulation_keywords = ['model', 'modelled', 'modeling', 'simulations', 'simulation', 'simulate']\n",
    "\n",
    "# Search for papers\n",
    "search = arxiv.Search(\n",
    "    query=query,\n",
    "    sort_by=arxiv.SortCriterion.SubmittedDate\n",
    ")\n",
    "search_results = search.results()\n",
    "\n",
    "# Filter search results by publication date and keywords\n",
    "filtered_results = []\n",
    "for paper in search_results:\n",
    "    if paper.published.date() < start_date:\n",
    "        continue\n",
    "    if any(keyword in paper.title.lower() or keyword in paper.summary.lower() for keyword in [\"brown dwarf\", \"white dwarf\"]):\n",
    "        continue\n",
    "    filtered_results.append(paper)\n",
    "\n",
    "# Write the results to a CSV file\n",
    "with open('step_1_helium_4.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['arXiv ID', 'URL', 'Exoplanet Names Found', 'Type', 'Type2', 'Type3'])\n",
    "\n",
    "    for paper in filtered_results:\n",
    "        arxiv_id = paper.entry_id.split('/')[-1]\n",
    "        url = f'https://arxiv.org/abs/{arxiv_id}'\n",
    "\n",
    "        # Find exoplanet names in the title and the abstract\n",
    "        exoplanet_names_title = {name for name in exoplanet_names if name in paper.title or name.replace(\" \", \"\") in paper.title}\n",
    "        exoplanet_names_abstract = {name for name in exoplanet_names if name in paper.summary or name.replace(\" \", \"\") in paper.summary}\n",
    "        exoplanet_names_found = exoplanet_names_title | exoplanet_names_abstract\n",
    "\n",
    "        # Determine the type of paper based on observation and simulation keywords\n",
    "        type = 'N'\n",
    "        if any(keyword in paper.title.lower() or keyword in paper.summary.lower() for keyword in observation_keywords):\n",
    "            type = 'O'\n",
    "        elif any(keyword in paper.title.lower() or keyword in paper.summary.lower() for keyword in simulation_keywords):\n",
    "            type = 'S'\n",
    "\n",
    "        # Determine the type2 of paper based on the presence of exoplanet names, but only if column 3 is filled\n",
    "        type2 = 'SS'\n",
    "        if exoplanet_names_found and exoplanet_names_found != set(['']):\n",
    "            type2 = 'E'\n",
    "\n",
    "        # Determine the type3 of paper based on the URL comparison\n",
    "        type3 = 'NO'\n",
    "        if url in helium_planets_urls:\n",
    "            type3 = 'HO'\n",
    "\n",
    "        # Write the result to the CSV file\n",
    "        writer.writerow([arxiv_id, url, ', '.join(exoplanet_names_found), type, type2, type3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a86b84b",
   "metadata": {},
   "source": [
    "## The code above also has many false positives that are not helium studies. Hence manually have to check them and shortlist the exact helium observation planets in the files step_2_helium_shortlisted.csv. If you change the name, please change the name in the code below as well.\n",
    "\n",
    "## The code below read the step_2_helium_shortlisted.csv file and arrange them in planets order (alphabetically) in the file step_3_helium_planets_update.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29c5bd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# read the data from the exoplanets file and store it in a dictionary\n",
    "exoplanet_data = {}\n",
    "with open('exoplanets.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    # get the header row\n",
    "    header = next(reader)\n",
    "    # iterate over the rows in the file\n",
    "    for row in reader:\n",
    "        # use the planet name as the key in the dictionary\n",
    "        planet_name = row[0]\n",
    "        # get the headers and data for the planet\n",
    "        planet_headers = header[2:]\n",
    "        planet_data = row[2:]\n",
    "        # store the data in a dictionary for the planet\n",
    "        exoplanet_data[planet_name] = (planet_headers, planet_data)\n",
    "\n",
    "# create a dictionary to store the planets and corresponding urls\n",
    "planet_urls = {}\n",
    "\n",
    "# read the data from the original file\n",
    "with open('step_2_helium_shortlisted.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    # skip the header row\n",
    "    next(reader)\n",
    "    # iterate over the rows in the file\n",
    "    for row in reader:\n",
    "        # split the planets by comma followed by a space\n",
    "        planets = [p.strip() for p in row[1].split(',')]\n",
    "        # iterate over the planets\n",
    "        for planet in planets:\n",
    "            # add the planet to the dictionary if it doesn't exist\n",
    "            if planet not in planet_urls:\n",
    "                planet_urls[planet] = []\n",
    "            # add the url to the list of urls for the planet\n",
    "            planet_urls[planet].append(row[0])\n",
    "\n",
    "# create a new file to write the data to\n",
    "with open('step_3_helium_planets_update.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    # get the headers from exoplanets.csv and add empty columns for columns 3-8\n",
    "    planet_headers = ['Planets', 'URL', 'excess_abs', 'excess_abs_min', 'excess_abs_max', 'upp_lim', 'mass_loss', 'mass_loss_min', 'mass_loss_max','mass_loss_upp'] + exoplanet_data[next(iter(exoplanet_data.keys()))][0]\n",
    "    # write the header row\n",
    "    writer.writerow(planet_headers)\n",
    "    # iterate over the planets in alphabetical order\n",
    "    for planet in sorted(planet_urls.keys()):\n",
    "        # get the data for the planet from the exoplanet data dictionary\n",
    "        if planet in exoplanet_data:\n",
    "            # get the headers and data for the planet\n",
    "            planet_headers, planet_data = exoplanet_data[planet]\n",
    "            # create a list of the data for the planet in the correct order\n",
    "            planet_row = [planet, ', '.join(planet_urls[planet]), '', '', '', '', '', '', '', ''] + planet_data\n",
    "            # write the row for the planet\n",
    "            writer.writerow(planet_row)\n",
    "        else:\n",
    "            # write a row with the planet and url, leaving the exoplanet data columns empty\n",
    "            writer.writerow([planet, ', '.join(planet_urls[planet]), '', '', '', '', '', '', '', '', '', '', '', '', '', ''])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3881898",
   "metadata": {},
   "source": [
    "## After this add the excess abs, excess abs min and max limits or upper limits in the step_3_helium_planets_for_plots.csv file manually . This is a crucial step. And manually copy these to new row in helium_planets_for_plots.csv or if the planets already exist, manually change the specific columns.\n",
    "\n",
    "## We will use helium_planets_for_plots.csv to create a different csv file to use for calculations. And we will store the file as helium_planets.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2abd5e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the original CSV file\n",
    "input_file = 'helium_planets_for_plots.csv'\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Select the first 10 columns\n",
    "selected_columns = df.iloc[:, :10]\n",
    "\n",
    "# Write the selected columns to a new CSV file\n",
    "output_file = 'helium_planets.csv'\n",
    "selected_columns.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c71e05",
   "metadata": {},
   "source": [
